# å¯¼å…¥å¿…è¦çš„åº“
import streamlit as st  # Streamlitç”¨äºåˆ›å»ºWebåº”ç”¨ç•Œé¢
import pandas as pd     # ç”¨äºæ•°æ®å¤„ç†å’Œåˆ†æ
import numpy as np      # ç”¨äºæ•°å€¼è®¡ç®—
import tensorflow as tf  # ç”¨äºåŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹
import matplotlib.pyplot as plt  # ç”¨äºæ•°æ®å¯è§†åŒ–
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc  # ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡
from matplotlib import rcParams  # ç”¨äºè®¾ç½®matplotlibçš„å­—ä½“å’Œæ ·å¼
import io  # ç”¨äºå¤„ç†å†…å­˜ä¸­çš„æ–‡ä»¶å¯¹è±¡
import os


# é…ç½®matplotlibçš„æ˜¾ç¤ºå‚æ•°
rcParams['font.family'] = 'sans-serif'  # è®¾ç½®å­—ä½“
rcParams['font.size'] = 12              # è®¾ç½®å­—ä½“å¤§å°

# è®¾ç½®Streamlité¡µé¢é…ç½®
st.set_page_config(
    page_title="External Validation Analysis",  # é¡µé¢æ ‡é¢˜
    page_icon="ğŸ“Š",                            # é¡µé¢å›¾æ ‡
    layout="wide",                             # ä½¿ç”¨å®½å±å¸ƒå±€
    initial_sidebar_state="expanded"           # åˆå§‹ä¾§è¾¹æ çŠ¶æ€ä¸ºå±•å¼€
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
    <style>
    .main {background-color: #f8f9fa;}  # ä¸»èƒŒæ™¯è‰²
    .stButton>button {background-color: #007bff; color: white; border-radius: 8px;}  # æŒ‰é’®æ ·å¼
    .metric-card {background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);}  # æŒ‡æ ‡å¡ç‰‡æ ·å¼
    h1 {color: #2c3e50; text-align: center;}  # æ ‡é¢˜æ ·å¼
    h2 {color: #34495e; border-bottom: 2px solid #17a2b8; padding-bottom: 5px;}  # äºŒçº§æ ‡é¢˜æ ·å¼
    </style>
""", unsafe_allow_html=True)

# é¡µé¢æ ‡é¢˜å’Œä»‹ç»
st.title("ğŸ“Š External Validation Analysis")
st.markdown("""
    æ­¤å·¥å…·ä½¿ç”¨ç‹¬ç«‹æ•°æ®é›†å¯¹é¢„æµ‹æ¨¡å‹è¿›è¡Œå¤–éƒ¨éªŒè¯ã€‚
    è®¡ç®—å…³é”®è¯„ä¼°æŒ‡æ ‡å¹¶ç”ŸæˆROCæ›²çº¿ä»¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
""")

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹


@st.cache_resource
def load_model():
    """åŠ è½½é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹"""
    return tf.keras.models.load_model('data/MODEL.h5')

# åŠ è½½éªŒè¯æ•°æ®


@st.cache_data
def load_validation_data():
    """åŠ è½½å¤–éƒ¨éªŒè¯æ•°æ®é›†"""
    df = pd.read_excel('data/merge_external_validation.xlsx')
    return df.iloc[:, :-2], df.iloc[:, -1]


# æ¨¡å‹çš„cutoffå€¼
def find_model_cutoff():
    df = pd.read_csv('data/testset_cutoff.csv')
    y_true = df.iloc[:, 0]
    # å¦‚æœæœ‰å¤šåˆ—ï¼Œåˆ™è®¡ç®—æ¯ä¸€åˆ—çš„cutoffå€¼
    # # åˆ›å»ºcutoffæ•°æ®æ¡†
    cutoff_df = pd.DataFrame()
    cutoff_df['model'] = df.columns[1:]
    y_pred = df.iloc[:, 1:]
    cutoff_values = []
    for col in y_pred.columns:
        fpr, tpr, thresholds = roc_curve(y_true, y_pred[col])
        cutoff = round(thresholds[np.argmax(tpr - fpr)], 3)
        cutoff_values.append(cutoff)
        cutoff_df[col] = cutoff
    return cutoff_df


# è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
def calculate_metrics(y_true, y_pred_proba, cutoff):
    # æ ¹æ®æ¨¡å‹çš„cutoffå€¼è®¡ç®—ROCæ›²çº¿
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)

    # å°†æ¦‚ç‡è½¬æ¢ä¸ºé¢„æµ‹æ ‡ç­¾ï¼ˆé˜ˆå€¼cutoffï¼‰
    y_pred = (y_pred_proba >= cutoff).astype(int)

    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    metrics = {
        'AUC': round(roc_auc, 3),  # AUC
        'Accuracy': round(accuracy_score(y_true, y_pred), 3),    # å‡†ç¡®ç‡
        'Precision': round(precision_score(y_true, y_pred), 3),  # ç²¾ç¡®ç‡
        'Recall': round(recall_score(y_true, y_pred), 3),        # å¬å›ç‡
        'F1 Score': round(f1_score(y_true, y_pred), 3),           # F1åˆ†æ•°
        'Cutoff': round(cutoff, 3)  # cutoff
    }

    return metrics, fpr, tpr, roc_auc, y_pred, cutoff


# ä¸»ç¨‹åº
def main():
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    model = load_model()
    X_val, y_true = load_validation_data()
    print(f"X_val: {X_val}")
    print(f"y_true: {y_true}")

    # ä¸»è¦å†…å®¹
    st.header("Model Performance Metrics")  # æ¨¡å‹æ€§èƒ½æŒ‡æ ‡

    # è®¡ç®—cutoffå€¼
    cutoff_df = find_model_cutoff()
    
    # æ˜¾ç¤ºcutoffæ•°æ®æ¡†
    st.write("Model Cutoff:")
    st.dataframe(cutoff_df)
        
    model_cutoff = cutoff_df.iloc[0, 1]
    print(f"model_cutoff: {model_cutoff}")

    # è¿›è¡Œé¢„æµ‹
    y_pred_proba = model.predict(X_val, verbose=0)

    # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œè®¡ç®—è¯„ä¼°æŒ‡æ ‡
    if y_true is not None:
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics, fpr, tpr, roc_auc, y_pred, cutoff = calculate_metrics(
            y_true, y_pred_proba, model_cutoff)

        # åœ¨ç½‘æ ¼ä¸­æ˜¾ç¤ºæŒ‡æ ‡ï¼Œå¢åŠ auc,acc,precision,recall,f1,cutoff
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        with col1:
            st.metric("AUC", f"{metrics['AUC']:.3f}")    # æ˜¾ç¤ºAUC
        with col2:
            st.metric("Accuracy", f"{metrics['Accuracy']:.3f}")    # æ˜¾ç¤ºå‡†ç¡®ç‡
        with col3:
            st.metric("Precision", f"{metrics['Precision']:.3f}")  # æ˜¾ç¤ºç²¾ç¡®ç‡
        with col4:
            st.metric("Recall", f"{metrics['Recall']:.3f}")        # æ˜¾ç¤ºå¬å›ç‡
        with col5:
            st.metric("F1 Score", f"{metrics['F1 Score']:.3f}")    # æ˜¾ç¤ºF1åˆ†æ•°
        with col6:
            st.metric("Cutoff", f"{cutoff:.3f}")    # æ˜¾ç¤ºcutoff
        # ç»˜åˆ¶ROCæ›²çº¿
        st.header("ROC Curve")
        fig, ax = plt.subplots(figsize=(8, 6))
        ax.plot(fpr, tpr, color='darkorange', lw=2,
                label=f'ROC curve (AUC = {roc_auc:.3f})')  # ç»˜åˆ¶ROCæ›²çº¿
        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # ç»˜åˆ¶å¯¹è§’çº¿
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate')  # è®¾ç½®xè½´æ ‡ç­¾
        ax.set_ylabel('True Positive Rate')   # è®¾ç½®yè½´æ ‡ç­¾
        ax.set_title('External Validation ROC Curve')  # è®¾ç½®æ ‡é¢˜
        ax.legend(loc="lower right")  # æ·»åŠ å›¾ä¾‹

        # ä¿å­˜ROCæ›²çº¿ä¸ºPDF
        buf = io.BytesIO()
        fig.savefig(buf, format='pdf', bbox_inches='tight', dpi=300)
        buf.seek(0)

        # æ˜¾ç¤ºROCæ›²çº¿
        st.pyplot(fig)

        # æ·»åŠ PDFä¸‹è½½æŒ‰é’®
        st.download_button(
            label="Download ROC Curve (PDF)",
            data=buf,
            file_name='roc_curve.pdf',
            mime='application/pdf'
        )

        # å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾å†…å­˜
        plt.close(fig)

        # åˆ›å»ºè¯„ä¼°ç»“æœæ•°æ®æ¡†
        st.header("Evaluation Results")

        # åˆ›å»ºæŒ‡æ ‡æ•°æ®æ¡†
        metrics_df = pd.DataFrame({
            'Metric': ['AUC', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Cutoff'],
            'Value': [metrics['AUC'], metrics['Accuracy'], metrics['Precision'],
                      metrics['Recall'], metrics['F1 Score'], metrics['Cutoff']]
        })

        # è®¾ç½®æ˜¾ç¤ºæ ¼å¼ä¸º3ä½å°æ•°
        pd.set_option('display.float_format', lambda x: '%.3f' % x)

        # æ˜¾ç¤ºæŒ‡æ ‡æ•°æ®æ¡†
        st.write("Model Performance Metrics:")
        st.dataframe(metrics_df)

        # åˆ›å»ºé¢„æµ‹ç»“æœæ•°æ®æ¡†
        results_df = pd.DataFrame({
            'True Label': y_true,  # çœŸå®æ ‡ç­¾
            'Predicted Label': y_pred.flatten(),  # é¢„æµ‹æ ‡ç­¾
            'Predicted Probability': y_pred_proba.flatten()  # é¢„æµ‹æ¦‚ç‡
        })

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœæ•°æ®æ¡†
        st.write("Prediction Results:")
        st.dataframe(results_df)

        # æ·»åŠ ä¸‹è½½æŒ‰é’®
        st.download_button(
            label="Download Evaluation Results",
            data=metrics_df.to_csv(index=False).encode('utf-8'),
            file_name='evaluation_metrics.csv',
            mime='text/csv',
        )

        st.download_button(
            label="Download Prediction Results",
            data=results_df.to_csv(index=False).encode('utf-8'),
            file_name='prediction_results.csv',
            mime='text/csv',
        )

    # é¡µè„š
    st.markdown("---")
    st.markdown(
        f"External Validation Analysis | Updated: {pd.Timestamp.now().strftime('%Y-%m-%d')}")


if __name__ == "__main__":
    main()
